\chapter{Non-classical transport in heterogeneous materials}
\label{chap:heterogeneousnonclassical}


\textit{What happens to a particle's memory when it crosses a material interface?}


This is the main question that Chapter \ref{chap:heterogeneousnonclassical} attempts to answer. Easy to pose, we will see that the respective answer fundamentally depends upon the underlying assumptions that are made to model heterogeneous materials. We will discuss why this is a relevant, non-trivial question in the context of non-classical particle transport and encounter a fundamental problem of mathematical modeling: Lacking real-world measurements or experimental data, we are limited to educated guesses and bona fide speculations. And while these are \textit{falsifiable} guesses and speculations,  it is beyond the scope of this work to actually execute the necessary experiments or measurements to either falsify or verify the subsequent  models.

Note, again, that the problem of modeling transport in the presence of heterogeneities is exclusively limited to non-classical particle transport where the governing equations have to encode a particle's memory.
As already mentioned in Chapter \ref{chap:nonclassicaltransportequations}, classical transport is memoryless in the sense that
\begin{align*}
	\text{Pr}\left(S>s+\Delta s| S>s \right) = \text{Pr}\left(S>\Delta s\right).
\end{align*}
Consequently, the path length distribution for a particle that faces an interface a distance $s^*\geq 0$ away is simply
\begin{align}
	p(s) = \begin{cases}
			\sigma_1 e^{-\sigma_1 s} \, & \text{if }s\leq s^*, \\
			\sigma_2 e^{-\sigma_1  s^* - \sigma_2  (s-s^*)} \, & \text{if }s> s^*,
\end{cases}
\label{eq:pdfclassical}
\end{align}
where $\sigma_1$ and $\sigma_2$ denote the cross sections in front of and behind the interface, respectively.
%We are formulating the path length distribution in terms of the distance since the last interaction (be it birth or scattering), because of



With non-classical transport dating back to the early 2000s (approximately), heterogeneous non-classical transport has only recently become a topic of greater interest \cite{camminady2017nonclassical,jarabo2018radiative,bitterli2018radiative} and an equivalent formulation of \eqref{eq:pdfclassical} is not proven.
%To date, there is no definitive theory that models non-classical particle transport across interfaces.

\section{The crux of particles crossing material interfaces}
Using the backward formulation of the generalized linear Boltzmann equation, the total cross section depends upon the distance that a particle has already traveled.  For simplicity, we consider the situation depicted in Figure \ref{fig:hetmaterial}, where there is only one interface and a particle, moving from left to right across that interface. Considered individually, the cross sections on the left and right side of the interface are $\sigma_1(s)$ and $\sigma_2(s)$, respectively. The key point here, is the word \textit{individually}, because it implies that we are able to model the heterogeneity by two cross sections that are the result of an individual, homogeneous consideration.

\newpage

If we for now assume that this is a valid model, several requirements  seem reasonable \cite{camminady2017nonclassical}:
\begin{enumerate}
	\item The resulting path length distribution should be a probability density function in the sense that it integrates to unity.
	\item If both cross sections are independent of $s$, we want to recover \eqref{eq:pdfclassical}.
	\item Up to the interface, the path length distribution should only depend on $\sigma_1(s)$.
\end{enumerate}
\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pictogram_old/stuff_has_to_reduce_if_the_interface_is_artificial/hetmaterial2.pdf}
	\caption{Two materials with cross sections $\sigma_1(s)$ and $\sigma_2(s)$, and a particle traversing from one side of the interface to the other. The material extends to the right indefinitely.}
	\label{fig:hetmaterial}
\end{figure}
The first point is merely a technicality. The second point follows from the fact that the non-classical transport equation reduces to the classical transport equation if the cross sections are independent of the distance that a particle has traveled. It can also be used as a way to check any proposed candidate for the path length distribution for the non-classical, heterogeneous case.
Lastly, since we consider the \textit{backward} formulation, the path length distribution should be agnostic to future materials and obey the third point.

Now, if the cross sections of non-classical materials depend on $s$, and a particle crosses an interface after having traveled a distance $s^*$ to reach a material with $\sigma_2(s)$, should it
experience $\sigma_2(s^*+\Delta s)$ or $\sigma_2(\Delta s)$ when traveling another distance $\Delta s$ in the new material? Per se, there is no compelling reason to evaluate the cross section of the new material at $s^* + \Delta s$ rather than at $\Delta s$. Arguing for the former implies that it is possible to relate the distance traveled in \textit{one }material to the likelihood of scattering in \textit{another }material; even when we allow to choose the materials completely independent from each other.

Things become more complicated when we consider one further requirement:
\begin{itemize}
	\item [4.] If $\sigma_1(s)\equiv \sigma_2(s)$, we want to recover the homogeneous case.
\end{itemize}
This seems like a reasonable prerequisite since, in the case of $\sigma_1(s) \equiv \sigma_2(s)$, the interface is only artificial and the material is therefore homogeneous. As a result, the cross section should be evaluated at $s^*+\Delta s$ rather than at $\Delta s$ since the latter implies a memory reset for which there is no reason since the material is homogeneous.

Summarizing, we end up with the following dilemma: For two materials with arbitrarily chosen cross sections $\sigma_1(s)$ and $\sigma_2(s)$, the memory should be reset when crossing the interface. If, however,  the two cross sections are identical, the memory should persist.

We proposed an expression for the path length distribution that solves the dilemma in \cite{camminady2017nonclassical}. However, as we will see in Section \ref{sec:defheterogeneity}, there are arguments against the fourth prerequisite, ultimately rendering the memory loss inevitable. The corresponding path length distribution is analyzed in Section \ref{sec:memoryresetting}.




\section{Defining heterogeneity}
\label{sec:defheterogeneity}

To discuss the definition of heterogeneity, we will repeatedly refer to the two situations that are depicted in Figure \ref{fig:twopossibilities}.  In Figure \ref{fig:pictogram2}, we see (in the upper half) a homogeneous non-classical material from a microscopic point of view. The arrangement of obstacles is that of the periodic Lorentz gas.\footnote{We only chose the periodic Lorentz gas as one instance of all possible non-classical materials because it is immediately clear that the depicted material is non-classical. The analysis, however, is not limited to the periodic Lorentz gas.}  Performing the Boltzmann-Grad limit (\ie $n\rightarrow \infty$ such that $n\cdot r = \text{constant}$ and ensemble averaging over all possible arrangements), we end up with a material that is (on the mesoscopic level) defined by its cross section and path length distribution.

In the heterogeneous case, illustrated in Figure \ref{fig:pictogram3}, the two materials are identical in the sense that obstacles are arranged in a lattice structure. But---and this is shown for the green and blue scatterers in Figure \ref{fig:pictogram3}---it is entirely possible that these two lattices do not align perfectly. In general, we observe two arrangements of obstacles which, when considered individually, are valid arrangements of scatterers, but do not resemble the homogeneous case. If we now perform the Boltzmann-Grad limit individually, both of the materials are defined (on the mesoscopic level) by the same cross section and path length distribution. But since we ensemble average both sides independently, a particle's memory should be reset when crossing the interface. Ultimately, we are left with a situation that looks homogeneous on the mesoscopic level, but does not equal the homogeneous material on the microscopic level.


A situation that exemplifies these theoretical considerations is that of two cubes being pushed together. With their crystalline, Lorentz-like structure, both cubes have the same, correlated ordering of atoms. At the interface, however, this two-cube-setup will look different from a single quadrilateral of the same dimensions. If we now place a light source such that photons move through the setup from one cube to the next, we have reproduced the conditions that we described in Figure \ref{fig:pictogram3}.


Presented first in Camminady et al. \cite{camminady2017nonclassical}, a simplified, less general scenario  was considered  that nevertheless contributes to the discussion of heterogeneous materials. Depicted in Figure \ref{fig:truefalsehet}, the two scenarios distinguish heterogeneities that are due to density fluctuations (Figure \ref{fig:falsehet}) and those that formed by combining structurally different obstacle arrangements (Figure \ref{fig:truehet}). The ansatz for the first scenario that was presented in  Camminady et al. \cite{camminady2017nonclassical} will proceed the more general, fully heterogeneous case in our discussion later on.




By now it is clear that the definition of heterogeneity depends on our perspective.
Whether a heterogeneous domain with cross sections $\sigma_1(s)$ and $\sigma_2(s)$ turns homogeneous if $\sigma_1(s) \equiv \sigma_2(s)$ depends on the circumstances. If we define a material solely by its cross sections, it should be homogeneous. If we instead consider the underlying microscopic situation, an interface remains and a path length distribution has to account for it.


Given these circumstances, we will provide {two different} types of approaches to express the distribution of path lengths in presence of (potentially) different cross sections. One formulation preserves the particle's memory when crossing an untrue ($\sigma_1(s) \equiv \sigma_2(s)$) interface and another formulation where this is not the case. Both formulations, however, will satisfy the first three requirements that we imposed.



\begin{figure}
	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pictogram_old/stuff_has_to_reduce_if_the_interface_is_artificial/pictogram_2}
		\caption{Ensemble averaging the whole domain yields a homogeneous material, defined by $p(s)$ and $\sigma_t(s)$.}
		\label{fig:pictogram2}
	\end{subfigure}

	\vspace{1cm}

	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pictogram_old/stuff_has_to_reduce_if_the_interface_is_artificial/pictogram_3}
		\caption{If both halves are ensemble averaged independently, they should still yield the same distribution and cross section when considered individually. When looking at the whole domain, however, it is not clear that this way of executing the Boltzmann-Grad limit has to yield the same result for the whole domain as found in Figure \ref{fig:pictogram2}.}
		\label{fig:pictogram3}
	\end{subfigure}
	\caption{The path length distribution and cross section are the outcome of the Boltzmann-Grad limit. This involves an ensemble average over all possible configurations of obstacles $C_\mu$, even for the periodic Lorentz gas. For example, obstacle positions could be translated or rotated. This holds true for other arrangements as well, \eg Poisson disk sampling, where obstacles are at least a prescribed minimum distance apart from another. The subscript $\mu$ denotes a singular instance out of all possible arrangements and $\langle \cdot \rangle_\mu$ is the respective ensemble average.}
	\label{fig:twopossibilities}
\end{figure}



\begin{figure}

	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pictogram_old/stuff_has_to_reduce_if_the_interface_is_artificial/falsehet}
		\caption{A heterogeneous material that differs only in density as the underlying structure of obstacles is identical.}
		\label{fig:falsehet}
	\end{subfigure}

	\begin{subfigure}{1.0\textwidth}
		\centering
		\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pictogram_old/stuff_has_to_reduce_if_the_interface_is_artificial/truehet}
		\caption{A heterogeneous material that comprises two fundamentally different types of obstacle arrangements.}
		\label{fig:truehet}
	\end{subfigure}
	\caption{Material heterogeneities can be the result of density fluctuations (Figure \ref{fig:falsehet}), or structural differences (Figure \ref{fig:truehet}).}
	\label{fig:truefalsehet}
\end{figure}

\FloatBarrier

\section{Memory-resetting ansatz}
\label{sec:memoryresetting}

Arguably the simpler---and maybe less exciting---case, we will start by considering the scenario in which the memory of a particle gets reset at an interface.

Consider a particle positioned at $x_0$ inside Material $1$ which is defined by $\sigma_1(s)$ and $p_1(s)$. For a fixed direction of flight, the particle crosses the interface between Material $1$ and Material $2$ at $x_0+s_1$. More generally, after traveling a distance $s_i$, the particle is crossing the interface between Material $i-1$ and Material $i$, where the $s_i$ are prescribed by the arrangement of materials; additionally, we define $s_0=0$. This situation is visualized for the direction $\bOmega = (1,0)^T$ in Figure \ref{fig:traverse}. The individual cross sections and path length distributions for a material describe a particle's dynamics if it were to be placed inside an infinite, homogeneous domain of exactly that material.

\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/traverse/traverse}
	\caption{A particle traversing multiple materials with different cross sections $\sigma_i(s)$ and related density functions $p_i(s)$ with interfaces located distances $s_i$ away from its origin. The cross sections and density functions are to be interpreted in the homogeneous sense, \ie if the particle would be in a homogeneous, infinite domain of Material $i$, it would experience cross section $\sigma_i(s)$ and density function $p_i(s)$.}
	\label{fig:traverse}
\end{figure}



A particle's memory is reset by either resampling the distance to the next interaction after the crossing of an interface, or by resetting the value of $s$ that is used to evaluate the probability to scatter via $\sigma_i(s)$.

Thus, the probability for a particle that has traveled a distance $s$ from $x_0$ to reach $x_0+s$ to scatter while traveling another distance $ds$ is
\begin{align}
	\sigma(s)ds = \sigma_{\chi(x_0+s)}\left(s-s_{\chi(x_0+s) -1}\right) ds,
	\label{eq:manysigma}
\end{align}
where the indicator function $\chi(x)$ returns the index of the material at position $x$. Likewise, we can express the probability that a particle at $x_0$ will undergo its next collision after traveling a distance $s$ to reach $x_0+s$ via
\begin{align}
%	\begin{split}
	p(s) = \begin{cases}
		p_1(s) & \text{ if } s \in [0,s_1),\\
		\left(1-\int_0^{s_1} p(s')\, ds'\right)p_2(s-s_1)& \text{ if } s \in [s_1,s_2), \\
		\left(1-\int_{0}^{s_2} p(s')\, ds'\right)p_3(s-s_2) &\text{ if } s\in [s_2,s_3),\\
		\left(1-\int_{0}^{s_3} p(s')\, ds'\right)p_4(s-s_3) &\text{ if } s\in [s_3,s_4),\\
		... &
	\end{cases}
 \label{eq:manypdf}
\end{align}
The factors involving an integral in every line but the first renormalize the probabilities to take into account that particles might have scattered in an earlier interval.
Domain-wise, the relation \eqref{eq:trafo2} holds. That this is also true for \eqref{eq:manysigma} and \eqref{eq:manypdf} demonstrates the following analysis.
First, we compute the respective cumulative distribution function for \eqref{eq:manypdf}. Assuming $s \in [s_{i-1},s_i)$, this becomes
\begin{subequations}
\begin{align}
	P(s) &\defas \int_0^s p(s') ds' \\
	       & = P(s_{i-1}) + \int_{s_{i-1}}^s p(s')\, ds'  \\
	       	& = P(s_{i-1}) + \int_{s_{i-1}}^s  \left(1-P(s_{i-1})\right) p_i(s'-s_{i-1})\, ds'   \\
	 %     	&= P(s_{i-1}) +  \left(1-P(s_i)\right)\int_{s_{i-1}}^s  p_i(s'-s_{i-1})\, ds' \\
	       	&= P(s_{i-1}) +  \left(1-P(s_{i-1})\right)  P_i(s-s_{i-1}).
\end{align}
\end{subequations}

We know from \eqref{eq:trafo2} that 	$\sigma(s) = p(s) / \int_s^\infty p(s')\, ds'$.
In our case, for $s \in [s_{i-1},s_i)$, this becomes
\begin{subequations}
\begin{align}
\sigma(s) &= \frac{\left(1-P(s_{i-1})\right)p_i(s-s_{i-1})}{1-\left[P(s_{i-1}) +  \left(1-P(s_{i-1})\right)  P_i(s-s_{i-1}) \right]} \\
&=
\frac{p_i(s-s_{i-1})}{1-P_i(s-s_{i-1})} \\
&= \sigma_{i}(s-s_{i-1}),
\end{align}
\end{subequations}
which is exactly the relation from \eqref{eq:manypdf}. Consequently, \eqref{eq:manysigma} and \eqref{eq:manysigma} are equivalent ways to describe the same behavior.

\begin{table}
	\centering
	\begin{tabular}{lccr}
		\toprule
		\hfill i \hfill & $p_i(s)$          & \hfill  $\sigma_i(s)$ & \hfill Name \hfill\\
		\midrule
		$1$                    & $e^{-s}$           &$ 1$                             & Exponential\\
		$2$                    & $2e^{-2s}$           &$ 2$                             & Exponential\\
		$3$       			& $se^{-s}$           &$\frac{s}{s+1} $& Gamma\\
		$4$       & $\begin{cases}
			0 \text{ if } s\leq s_0, \\
			\alpha \frac{s_0^\alpha}{s^{\alpha+1}} \text{ if } s>s_0
		\end{cases}$           & $\begin{cases}
			0 \text{ if } s\leq s_0,\\
			\frac{\alpha}{s}  \text{ if } s>s_0
		\end{cases}$ & Pareto  ($\alpha =3$, $s_0=1$)\\

		\bottomrule
	\end{tabular}
	\caption{Cross sections and distribution functions for the materials in Figure \ref{fig:pdfvssigma}.}
	\label{tab:materials}
\end{table}



By design, this formulation does not reproduce the non-classical, homogeneous formulation if the cross sections or distribution functions are identical. As an example, Figure \ref{fig:pdfvssigma} shows different materials which are explained in Table \ref{tab:materials}.
Here, the Pareto distribution has both a finite mean and a finite variance for $\alpha \geq 2$ which is why we choose $\alpha=3$ in this case.
In Figure \ref{fig:combinedpdf1}, we have two domains, both consistent of the same material; path lengths are  gamma distributed.
This is also indicated by the color coding; purple corresponds to $p_3(s)$ in Figure \ref{fig:pdfvssigma}.
 The combined path length distribution is then made up of two parts: For $s\leq 2$, particles only experience the first material and the combined path length distribution $p(s)$ is identical to the gamma distribution.
 However, when particles cross the interface at $s=2$, their memory is reset.
 The probability to further travel a certain distance, given that the particle has already traveled a distance $s=2$ is re-sampled from the path length distribution of the new domain.
 Hence, the combined (unconditional) distribution has a discontinuity at $s=2$.
 Nevertheless, $p(s)$ still integrates to unity (as required for any probability density function).

 If we had chosen both materials as classical materials, the combined distribution would be equivalent to the distribution for the homogeneous case. For non-classical materials, however, this is not the case.

 Lastly, we consider a more elaborate scenario in Figure \ref{fig:combinedpdf23322} with two interfaces and three distinctively different materials. The domains' colors again correspond to the materials presented in Figure \ref{fig:pdfvssigma}.  As before, the path length distribution results from the application of  \eqref{eq:manypdf} and is visualized in Figure \ref{fig:combinedpdf2}.
Similarly, we can use \eqref{eq:manysigma} to compute the resulting $s$-dependent cross section, shown in Figure \ref{fig:combinedpdf3}.
At every interface, the particle's distance to the next collision is resampled from the respective distribution and the cross section ignores the distance traveled up to the interface, \ie the memory is reset.







\begin{figure}
	\centering
	\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pdfs/pdfvssigma}
	\caption{Different materials defined by their density functions and cross sections. Here, $p_1$ and $p_2$ are exponential distributions, $p_4$ is a gamma distribution, and $p_4$ is a Pareto distribution. Cross sections are computed with \eqref{eq:trafo2}.}
	\label{fig:pdfvssigma}
\end{figure}




\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{Chapters/Chapter5/figures/pdfs/combinedpdf1}
	\caption{Combined path length distribution for a domain with two materials that are identical; both have gamma distributed path lengths.}
	\label{fig:combinedpdf1}
\end{figure}
\begin{figure}
\begin{subfigure}{1\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{Chapters/Chapter5/figures/pdfs/combinedpdf2}
	\caption{Combined path length distribution.}
	\label{fig:combinedpdf2}
\end{subfigure}
\begin{subfigure}{1\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{Chapters/Chapter5/figures/pdfs/combinedpdf3}
	\caption{Combined cross section.}
	\label{fig:combinedpdf3}
\end{subfigure}
\caption{Distribution and cross section for a domain composed of three materials: \newline Material $1$ is defined by a gamma distribution, Material $2$ by an exponential distribution, and Material $3$ by a Pareto distribution.}
\label{fig:combinedpdf23322}
\end{figure}




\FloatBarrier


\begin{table}[h!]
	\centering
	\begin{tabular}{lccr}
		\toprule
		\hfill Name \hfill & General definition    $p_\rho(s)$     & \hfill  Base density $p_1(s)$ & Conversion $\rho \cdot \left(p_1(\rho \cdot s)\right)$\hfill  \hfill\\
		\midrule
		Exponential                 &$\rho e^{-\rho s}$  & $e^{-s}$        & $\rho \cdot \left(e^{-\rho s}\right)$\\
		Gamma &  $\rho^2 s e^{-\rho s}		                  $ & $se^{-s}$                 & $\rho \cdot \left( \rho \, s e^{-\rho s}\right)$\\
		Pareto & $\begin{cases}
			0 \text{ if } s\leq 1/\rho, \\
			\alpha \frac{(1/\rho)^\alpha}{s^{\alpha+1}} \text{ if } s> 1/\rho
		\end{cases}		                  $ & $\begin{cases}
			0 \text{ if } s\leq 1, \\
			\alpha \frac{1}{s^{\alpha+1}} \text{ if } s>1
		\end{cases}$                 &  $\rho \cdot \begin{cases}
			0 \text{ if } \rho \, s  \leq 1, \\
			\alpha \frac{1}{s^{\alpha+1}\rho^{\alpha+1}} \text{ if } \rho \, s  > 1
		\end{cases}		                  $\\
		\bottomrule
	\end{tabular}
	\caption{Requirement (ii) is satisfied for all probability density functions, showcased
		for those functions that we have already encountered. For each one, we observe that the general definition equals the conversion. The $\bx$-dependency of $\rho$ was only omitted to shorten notation.}
	\label{tab:materialssatisfybase}
\end{table}

\section{Memory-preserving ansatz}
\label{sec:memorypreserving}

Next, we are discussing memory-preserving ans√§tze. We start with the simpler scenario that was initially considered in Camminady et al. \cite{camminady2017nonclassical} and in which heterogeneity is the result of density fluctuations of a single type of material.

\subsection{Heterogeneity via density fluctuations}

In this scenario, the fundamental idea is similar to the conversion between the geometric and optical path length in optics.	Light that travels a distance $s$ through a medium has a geometric path length of $s$. The optical path length, however, takes the refractive index $n$ of that material into account and is computed as the product of $s$ and $n$. Then, to sample the optical path length, we only need to sample values from the geometric path length and divide them by $n$.

We make the following two assumptions for the conversion to work in our application. (i) We are equipped with a \textit{base} density function for the distance to the next collision, called $p_1(s)$. (ii) If the material found at $\bx$ were to cover the full, infinite, homogeneous domain, its density function could be written as $p_{\rho(\bx)}(s) \defas \rho(\bx)\, p_1(\rho(\bx) \cdot s)$ for some function $\rho(\bx)$ that encodes the domain's density fluctuations.
Note that (ii) is a reasonable assumption for all probability density functions,  shown in Table \ref{tab:materialssatisfybase} for the distributions we have encountered so far.
Moreover, with $P_{\rho(\bx)}(s)$ and $P_1(s)$ as the cumulative distribution functions, $P_{\rho(\bx)}(s) = P_1(\rho(\bx) \cdot s)$ holds. It is important to emphasize again that this is not the distribution for the full heterogeneous case, but only describes the geometric-to-optical path length translation if the whole domain were to be filled with the material found at $\bx$.  However, the structure of this expression allows us to make an ad hoc ansatz for the distribution of a heterogeneous domain---where the heterogeneity is still the result of density fluctuations.

\begin{conjecture}[Path length distribution for the heterogeneous case with density fluctuations]
	Assume that a particle underwent its last collision at $\bx$, now facing direction $\bOmega$. Considered individually in a homogeneous setting, the material at $\bx$ has a distribution for the distance to the next collision $p_{\rho(\bx)}(s)$ that satisfies $ p_{\rho(\bx)}(s) =\rho(\bx)\, p_1(\rho(\bx) \cdot s)$ for a known $p_1(s)$ and $\rho(\bx)$ that describe a base distribution and density fluctuations, respectively. Then, the probability for that particle to travel a distance smaller than or equal to $s$ until its next collision is given by
	\begin{align}
		P(s,\bx,\bOmega) =  P_1\left(\int_0^s \rho(\bx+s'\cdot \bOmega)\, ds'\right).
		\label{eq:hetcdf}
	\end{align}
\end{conjecture}
Consequently, the respective density function satisfies
\begin{align}
	p(s,\bx,\bOmega) = \rho(\bx + s\cdot\bOmega) \, p_1\left(\int_0^s \rho(\bx+s'\cdot \bOmega)\, ds'\right)
	\label{eq:hetpdf}
\end{align}
and it follows that the  cross section satisfies
\begin{align}
	\sigma(s,\bx,\bOmega) = \frac{ \displaystyle \rho(\bx + s\cdot\bOmega) \, p_1\left(\int_0^s \rho(\bx+s'\cdot \bOmega)\, ds'\right)}{\displaystyle 1-P_1\left(\int_0^s \rho(\bx+s'\cdot \bOmega)\, ds'\right)}.
	\end{align}
It is eminent that \eqref{eq:hetcdf} and \eqref{eq:hetpdf} reduce to the known results in the case of (i) a classical, but heterogeneous material, \ie $p_1(s) = e^{-s}$, and (ii) a non-classical, but homogeneous material, \ie $\rho(\bx+s\cdot\bOmega) = \sigma$.

In Figure \ref{fig:materialoverview}, we see the expression \eqref{eq:hetpdf} for different densities $\rho_1(s) = \left(1 \text{ if } s\leq 2 \text { else } 2\right)$, $\rho_2(s) = \left(2 \text{ if } s\leq 2 \text { else } 1\right)$, and $\rho_3(s) \equiv 1.5$. The base distribution is the gamma distribution (Figure \ref{fig:allmaterials}) or the Pareto distribution (Figure \ref{fig:allmaterialspareto}). For $\rho_3(s)$, the respective path length distribution equals the homogeneous one since we preserve the particles' memory.





\begin{figure}[h!]
\begin{subfigure}{1\textwidth}
	\centering
	\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pdfsdensityfluctuations/gammapdf}
	\caption{Gamma distribution as $p_1(s)$.}
	\label{fig:allmaterials}
\end{subfigure}
\begin{subfigure}{1\textwidth}
	\centering
	\includegraphics[width=1.0\linewidth]{Chapters/Chapter5/figures/pdfsdensityfluctuations/paretopdf}
	\caption{Pareto distribution ($\alpha=3)$ as $p_1(s)$. The $y$-axis is log-scaled to better capture the discontinuity at $s=2$.}
	\label{fig:allmaterialspareto}
\end{subfigure}
	\caption{Path length distributions for a heterogeneous domain with a single interface and varying base distribution. The different lines show the distributions for different density fluctuations.}
\label{fig:materialoverview}
\end{figure}


\subsection{True heterogeneity}
The obvious drawback of the previous approach is that its application is limited to materials whose heterogeneity has to be explainable by density fluctuations only. Domains composed of
\textit{structurally} different materials are not governed by \eqref{eq:hetpdf}.

To overcome this limitation, we need to be able to stitch together arbitrary path length distributions.
This kind of  \textit{surgery} needs to be done carefully to both preserve a particle's memory and
ensure that the resulting path length distribution still integrates to unity.
The approach presented here ensures this by stitching together the materials' cumulative distribution functions in a way that ensures continuity of the overall, fully heterogeneous cumulative distribution.
This is done in the following way: A particle that has already traveled a certain distance $s'$ is bound to enter a new material at $\bx + s' \cdot  \bOmega$. If we know the value of the heterogeneous cumulative distribution function $P(s,\bx,\bOmega)$ up to the point $s=s'$, we can try to find $t'$ such that $P(s',\bx,\bOmega) = P_{\bx+s'\cdot \bOmega}(t')$. Here, $P_{\bx+s'\cdot \bOmega}(t')$ is the cumulative distribution function of the material that the particle is about to enter. Next, we stitch the part of $P_{\bx+s'\cdot \bOmega}(t)$ where $t\geq t'$  to  the right of $P(s',\bx,\bOmega) $. Once the particle enters another material, the process repeats itself.

Finding the value of $t'$ such that $P(s',\bx,\bOmega) = P_{\bx+s'\cdot \bOmega}(t')$ is equivalent to asking the question: How far could a particle have traveled inside the material at $\bx+s' \cdot \bOmega$ to reach the same value of the cumulative distribution function $P(s,\bx,\bOmega)$ that we currently observe?
With this intuitive understanding of the process, we are now ready to reformulate it in a more precise, mathematical language.

As before, we assume that $p_{\bx}(s)$ defines the probability density function for the distance to the next collisions in an infinite, homogeneous domain that is filled with the material found at $\bx$. Similarly, we interpret $P_\bx(s)$. A cumulative distribution function, $P_\bx(s)$ is invertible on $[0,1]$ with $P_\bx^{-1} : [0,1] \to \Rgeqz$. Abbreviate $\bx(s) = \bx + s \cdot \bOmega$; even tough we can assume $\bx=\mathbf{0}$ and $\bOmega = \mathbf{e}_1$ without loss of generality. The expression we are interested in is $p(s,\bx,\bOmega)$, interpreted as
\begin{align*}
	p(s,\bx,\bOmega)
	=&  \text{ ``the probability that a particle  which underwent a collision at } \bx
	\\ &\text{ \, and faces direction } \bOmega \text{ will travel a distance } s \text{ through a }
	\\ & \text{ \, truly heterogeneous material to scatter at  } \bx + s \cdot \bOmega."
\end{align*}

For a given value $P(s,\bx,\bOmega)$, we find $t$ such that $P_{\bx(s)}(t) = P(s,\bx,\bOmega)$ via
\begin{align}
	t = P_{\bx(s)}^{-1}\left ( P(s,\bx,\bOmega) \right).
 \end{align}
Stitching the right half of $P_{\bx(s)}(t)$ to $P(s,\bx,\bOmega)$ implies that, for an infinitesimal distance $\Delta s$, we update
\begin{subequations}
\begin{align}
	P(s + \Delta s, \bx,\bOmega) &= P(s,\bx,\bOmega) + \Delta s \cdot p_{\bx(s)}(t) \\
	&=P(s,\bx,\bOmega) + \Delta s \cdot p_{\bx(s)}( P_{\bx(s)}^{-1}\left ( P(s,\bx,\bOmega) \right)).
\end{align}
\end{subequations}
Rearranging terms, letting $\Delta s \rightarrow 0$, and noting that $P(0,\bx,\bOmega)=0$, we obtain
an ordinary differential equation (ODE) for $P(s,\bx,\bOmega)$ in the form of
\begin{align}
	\begin{cases}
		P(0,\bx,\bOmega) &= 0, \\
		\frac{d}{ds}P(s,\bx,\bOmega) &= p_{\bx(s)}( P_{\bx(s)}^{-1}\left ( P(s,\bx,\bOmega) \right)).
	\end{cases}
\label{eq:ode}
\end{align}




Solving the above system for $P(s,\bx,\bOmega)$ and differentiating with respect to $s$ we obtain
\begin{align}
	p(s,\bx,\bOmega)=\frac{d}{ds}P(s,\bx,\bOmega)=p_{\bx(s)}(t)= p_{\bx(s)}\Bigl(
	P^{-1}_{\bx(s)}	\bigl(
P(s,\bx,\bOmega)
	\bigr)
	\Bigr),
\end{align}
\ie a distribution for traveling distance $t$ in a material defined by the
properties at $\bx(s)$. Since $p_{\bx(s)}(t)$ was chosen exactly such that it
represents the probability of traveling distance $s$ in the truly heterogeneous
material, we know that we have to sample the distance to collision for a
particle at $\bx$ facing direction $\bOmega$ from
\begin{align}
 p_{\bx(s)}\Bigl(
P^{-1}_{\bx(s)}	\bigl(
P(s,\bx,\bOmega)
\bigr)
\Bigr)
	\label{eq:PDF}
\end{align}
with $P(s,\bx,\bOmega)$ as the solution of \eqref{eq:ode}.
In practice this means that for every particle in a heterogeneous domain, we
have to solve \eqref{eq:ode} to generate a sample from the probability density function for the
distance to collision. However, there are two cases
where we can avoid the high computational costs for sampling: First in the case
of generating multiple samples from one single distribution. This would be the case when
sampling the distance to collision for particles emitted by a source, all
facing the same direction. The second case deals with piecewise homogeneous
materials. If we loosen the assumption of continuously varying material and
assume the material to be piecewise homogeneous---which is a reasonable modeling assumption---we can avoid solving \eqref{eq:ode}. Instead we
march along the trajectory of the particle and only need to evaluate the cumulative distribution function and its inverse
for the distance to collision of each material which we
assumed to be known in the first place.

We can again validate that the generalized expression presented here reduces to the know results for simpler cases.

{\bfseries{Is $p(s,\bx,\bOmega)$  a probability density function?}}

At every point $\bx$, we know that $p_{\bx}(s)$ is a probability density function. The mapping from $s$
to $t$ is one-to-one ($t$ to $P(s,\bx,\bOmega)$ is one-to-one and $P(s,\bx,\bOmega)$ to $s$ is
one-to-one). Thus, with $P(s,\bx,\bOmega)=p_{\bx(s)}(t)$, also $P(s,\bx,\bOmega)$ is a probability density function.

{\bfseries{Is the case of a classical material treated correctly?}}


In the classical, possibly heterogeneous case, we have $p_{\bx(t)}(s) = \rho(\bx(t))
e^{-\rho(\bx(t))s}$, $P_{\bx(t)}(s) = 1-e^{-\rho(\bx(t))s}$, and consequently $P_{\bx(t)}^{-1}(y) = -\log(1-y)/\rho(\bx(t))$.
We compute
\begin{align}
	p_{\bx(s)}\Bigl(
	P_{\bx(s)}^{-1}	\bigl(
	P(s,\bx,\bOmega)
	\bigr)
	\Bigr) = \rho({\bx(s)})(1-P(s,\bx,\bOmega))
\end{align}
via the inverse function theorem (IFT).
The resulting ODE system is then
\begin{align}
	\begin{cases}
		P(0,\bx,\bOmega)             & = 0,                                \\
		\frac{d}{ds}P(s,\bx,\bOmega) & = \rho(\bx(s)) (1-P(s,\bx,\bOmega)),
	\end{cases}
\end{align}
with solution
\begin{align*}
	P(s,\bx,\bOmega) = 1-e^{-\int_0^s \rho({\bx(s')})\, ds'}.
\end{align*}
The resulting probability density function $p(s,\bx,\bOmega)$ is thus given by
\begin{align*}
	p(s,\bx,\bOmega) = \rho(x(s)) e^{-\int_0^s \rho(x(s'))\, ds'},
\end{align*}
which matches the known results for the classical heterogeneous case.



{\bfseries{How is the non-classical case treated?}}

Next, let us consider a material that is heterogeneous because of its density fluctuations and therefore not truly heterogeneous.
 For this scenario, the distribution
 \begin{align*}
 	p(s,\bx,\bOmega) = \rho(\bx + s\cdot\bOmega) \, p_1\left(\int_0^s \rho(\bx+s'\cdot \bOmega)\, ds'\right)
\end{align*}
alongside the scaling
\begin{align}
	p_{\rho(\bx)}(s) = \rho(\bx)\, p_1(\rho(\bx) \cdot s)
	\label{eq:step1a}
\end{align}
was introduced previously. Integrating the last expression yields $	P_{\rho(\bx)}(s) = P_1(\rho(\bx) \cdot s)$ with an inverse
\begin{align}
	P^{-1}_{\rho(\bx)}(y) = P_1^{-1}( y) / \rho(\bx).
	\label{eq:step2a}
\end{align}
Abusing notation, we use $p_\bx(s)$ and  $p_{\rho(\bx)}(s)$ interchangeably.
The ODE then satisfies
\begin{align*}
	\frac{d}{ds}P(s,\bx,\bOmega) & =  p_{\bx(s)}\Bigl(
P_{\bx(s)}^{-1}
		\bigl(
	P(s,\bx,\bOmega)
	\bigr)
	\Bigr)                                                                                                  \\
	&
	= \rho(\bx(s))\, p_1\Bigl(
	\rho(\bx(s))
	P_{\bx(s)}^{-1}
		\bigl(
	P(s,\bx,\bOmega)
	\bigr)
	\Bigr)
	\quad \text{ by \eqref{eq:step1a}}                                                                    \\
	& = \rho(\bx(s)) \, p_1\Bigl(
	P^{-1}	\bigl(
	P(s,\bx,\bOmega)
	\bigr)
	\Bigr)
	\quad \text{ by \eqref{eq:step2a}}                                                                    \\
	& = \rho(\bx(s)) \frac{1}{(P^{-1})'(P(s,\bx,\bOmega))} \quad \text{ by IFT.}
\end{align*}
Rearranging terms and integrating yields
%Further transformations yield the expression from \eqref{eq:mchetero}:
\[
\begin{aligned}
	&  &(P^{-1})'(P(s,\bx,\bOmega))\cdot \frac{d}{ds} P(s,\bx,\bOmega) & = \rho(\bx(s))                                            \\
	\Leftrightarrow &       & \frac{d}{ds} P^{-1}(P(s,\bx,\bOmega))                         & = \rho(\bx(s))                                            \\
	\Leftrightarrow &       & P^{-1}(P(s,\bx,\bOmega))                                      & = \int_0^s\rho(\bx(s'))\, ds'                              \\
	\Leftrightarrow &       & P(s,\bx,\bOmega)                                              & = P\left(\int_0^s\rho(\bx(s'))\, ds'\right)  .
	%           \\
	%\Leftrightarrow &       & P(s,\bx,\bOmega)                                              & = \rho(\bx(s)) %q\left(\int_0^s\sigma(x(s'))\, ds'\right).
\end{aligned}
\]
Hence, for heterogeneity that results from density fluctuations, we recover the expression that we derived previously---translating the geometric distance to the optical distance by means of a base distribution.


\newpage

{\bfseries{Is void treated correctly?}}

Void regions imply a probability of zero to undergo collisions.
Consequently, we have to modify the ODE system since, for the probability density function $p_{\bx(t)}(s)\equiv 0$, there is no corresponding cumulative distribution function or an inverse of it. However, because the
right-hand side of the ODE system evaluates the probability density function which maps all inputs to
zero, a natural modification of the system is given by setting
\begin{align}
	P_{\bx(s)}\left(P_{\bx(s)}^{-1}(y)\right ) \defas0
	\label{eq:modODEvoid}
\end{align}
for any  void region. In this case, the value of $P(s,\bx,\bOmega)$ will not increase
within a void region, corresponding to a zero probability of scattering events.

Another subtlety arises in the case of a particle being sourced inside a material that satisfies $p_{\bx(0)}(0) = 0$, \ie a zero probability of undergoing a collision immediately at the start of the particle's trajectory. With the previous ODE system
\begin{align}
	\begin{cases}
		P(0,\bx,\bOmega)             & = 0       \\
		\frac{d}{ds}P(s,\bx,\bOmega) & = p_{\bx(s)}\Bigl(
				P_{\bx(s)}^{-1}	\bigl(
		P(s,\bx,\bOmega)
		\bigr)
		\Bigr)
	\end{cases}
\end{align}
and with the assumption $p_{\bx(0)}(0) = 0$ and the initial condition $P(0,\bx,\bOmega)= 0$ we obtain $P(s,\bx,\bOmega)\equiv 0$.
However, $p_{\bx(0)}(0) = 0$ has to be non-zero. Since $s=0$, the last collision must have taken place at $\bx(0) = \bx$. Thus, with $\bOmega$ as the direction of travel \textit{away} from the last collision, $-\bOmega$ points \textit{towards} the obstacle that caused the collision at $\bx$, causing the particle to collide instantaneously. Consequently, the probability to collide at $s=0$ has to be nonzero.



{\bfseries{Is the case of bounded domains treated correctly?}}

We restricted ourselves to unbounded domains so far. However, bounded domains can be handled similarly.
When a particle reaches the boundary $\partial V$ of the domain $V$, two scenarios are possible. Either the particle leaves the computational domain or it interacts with the boundary $\partial V$. In both cases, $P(s,\bx,\bOmega)$ is only valid for  $s<s^*$ where $s^*$ is chosen such that $\bx+s^*\cdot \bOmega \in \partial V$. Thus, before sampling the distance to the next collision, with probability $1-P(s^*,\bx,\bOmega)$, a particle is placed at the boundary (where it either interacts with the boundary or leaves the domain) and with probability $P(s^*,\bx,\bOmega)$ a sample $s$ from $P(s,\bx,\bOmega)$ for $s<s^*$ has to be generated as the distance to the next collision. However, this procedure is not specifically related to non-classical or heterogeneous transport.




%\subsection{Boundary and initial conditions}
%reference the paper from the CG community.


\section{Sampling path lengths}





When generating samples from the distribution for the distance to the next collision, two scenarios have to be distinguished: (i) Generating multiple samples from a single distribution, and (ii) generating one sample per distribution for several distributions. The first scenario corresponds to a situation where many particles are emitted at the same position, facing the same direction (\eg due to a source). The second, computationally more expensive scenario is relevant when many particles face different directions. For example, these particles may have been emitted by an isotropic source, thus the trajectories of the particles pass through different material compositions for different directions.
In both scenarios, samples will be generated via inverse transform sampling.
Here, we can again distinguish two cases. In the general case, the material composition might be different for every point $\bx(t)$ along the trajectory. The second case, which is more reasonable in applications, assumes that a particle faces a trajectory along which the material is piecewise homogeneous, illustrated in Figure \ref{fig:traverse}.




\subsection{The general case}
\FloatBarrier
In both cases we will assume that $p_{\bx(t)}(s), P_{\bx(t)}(s),$ and $P^{-1}_{\bx(t)}(s)$ are available for all materials within the domain.
To generate a single sample for the trajectory of one particle, the evolution equation for $P(s,\bx,\bOmega)$ has to be solved until a value $u$ is reached which was sampled from a uniform distribution on $[0,1]$ in advance. This procedure is put into pseudocode in Algorithm \ref{alg:singlesample}.
Undeniably too expensive, we provide this algorithm only for completeness. More relevant for practical applications are Algorithms 	\ref{alg:multiplesample} and 	\ref{alg:singlesamplegeneral}.
\begin{algorithm}[]
	\centering
	\caption{Generating a single sample in the general case.}

\begin{algorithmic}[1]
	\Function{SingleSample}{$\bx(t),p_{\bx(t)}(s), P_{\bx(t)}(s), P^{-1}_{\bx(t)}(s)$ }

%	\KwInput{The trajectory $x(t)$ of a particle and $q(x(t),s)$, $Q(x(t),s)$ and $Q^{-1}(x(t),s)$ along the trajectory.}
%	\KwOutput{A sample $s$ for the distance-to-collision.}
	%\KwData{Testing set $x$}
	%$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}

	%\tcc{Now this is an if...else conditional loop}
	%${}$\\
	\State $u \sim \mathcal{U}[0,1]$ %// Generate a sample from a uniform distribution.

	\State Solve \eqref{eq:ode} for $s^*$ such that $P(s^*,\bx,\bOmega) = u$

	\State \textbf{return}  $s^*$
%	Sample $u$ from $\mathcal{U}[0,1]$ \tcp*{Generate a sample from a uniform distribution.}
%	Solve \eqref{eq:ode} for $P(x_0,\bOmega,s^*) = u$. \tcp*{Solve the ODE until reaching the value $u$.}
%	${}$\\
	%\Return{$s^*$}
	\EndFunction
\end{algorithmic}
	\label{alg:singlesample}
\end{algorithm}



If we instead decide to generate multiple samples from a single distribution, we can solve the ODE once and evaluate the inverse of $P(s,\bx,\bOmega)$ (inverse with respect to the first argument) arbitrarily often afterwards. Since we evaluate $P^{-1}( u,\bx,\bOmega)$ for $u\in[0,1)$, we have to solve the ODE up until a final distance $P(s^*,\bx,\bOmega) = 1-\varepsilon$ for some $0<\varepsilon\ll1$. Note that we can not solve the ODE up until $P(s^*,\bx_0,\bOmega) = 1$, since this would be the case for $s^* = \infty$. From the solution of the ODE, we can create a lookup table that lets us evaluate $P^{-1}(u,\bx,\bOmega)$. Following this precomputation, the requested number of samples can be generated by evaluating $P^{-1}(u_i,\bx,\bOmega)$ for $u_i$ sampled from $\mathcal{U}[0,1-\varepsilon]$ and $i$ ranging from one to the number of requested samples, illustrated in Algorithm \ref{alg:multiplesample}.





\begin{algorithm}[]
	\centering
	\caption{Generating multiple samples in the general case.}

	\begin{algorithmic}[1]
		\Function{MultipleSample}{$\varepsilon,n,\bx(t),p_{\bx(t)}(s), P_{\bx(t)}(s), P^{-1}_{\bx(t)}(s)$ }

		%	\KwInput{The trajectory $x(t)$ of a particle and $q(x(t),s)$, $Q(x(t),s)$ and $Q^{-1}(x(t),s)$ along the trajectory.}
		%	\KwOutput{A sample $s$ for the distance-to-collision.}
		%\KwData{Testing set $x$}
		%$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}

		%\tcc{Now this is an if...else conditional loop}
		%${}$\\
		%\State $u \sim \mathcal{U}[0,1]$ %// Generate a sample from a uniform distribution.

		\State Solve \eqref{eq:ode} up to $s^*$ such that $P(s^*,\bx,\bOmega)  \leq 1-\varepsilon$
		\State $T = \text{lookup table}(P^{-1}(u,\bx,\bOmega))$ for $u$ from $0$ to $1-\varepsilon$
		\State $y = \text{zeros}(n)$

		\For{$i=1,\cdots,n$}
			\State $u_i \sim \mathcal{U}[0,1-\varepsilon]	$
			\State $y[i] = T(u_i)$ %// Evaluate the lookup-table
		\EndFor

		\State \textbf{return} $y$
		%	Sample $u$ from $\mathcal{U}[0,1]$ \tcp*{Generate a sample from a uniform distribution.}
		%	Solve \eqref{eq:ode} for $P(x_0,\bOmega,s^*) = u$. \tcp*{Solve the ODE until reaching the value $u$.}
		%	${}$\\
		%\Return{$s^*$}
		\EndFunction
	\end{algorithmic}
	\label{alg:multiplesample}
\end{algorithm}



\subsection{Sampling in piecewise homogeneous materials}
For piecewise homogeneous, non-classical materials, the computational costs for generating samples reduces dramatically. In the following, we will derive a formulation for the sampling procedure that allows to generate samples for the distance to the next collision for particles by iteratively tracking their trajectories through the spatial domain without having to solve the previous ODE system.


\begin{algorithm}[]
	\centering
	\caption{Generating a single sample in the piecewise homogeneous case (without void).}

	\begin{algorithmic}[1]
		\Function{SingleSamplePiecewise}{$s_i,\, p_{i}(s),\, P_{i}(s), \, P^{-1}_i(s)$ for all materials $i$ \newline$ {}\quad \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \, \, \, {}$ along the particle's trajectory}

		\State $i=0$, $y=0.0$, $t=0.0$
		\State $u \sim \mathcal{U}[0,1]$
		%	\KwInput{The trajectory $x(t)$ of a particle and $q(x(t),s)$, $Q(x(t),s)$ and $Q^{-1}(x(t),s)$ along the trajectory.}
		%	\KwOutput{A sample $s$ for the distance-to-collision.}
		%\KwData{Testing set $x$}
		%$\sum_{i=1}^{\infty} := 0$ \tcp*{this is a comment}

		%\tcc{Now this is an if...else conditional loop}
		%${}$\\
		%\State $u \sim \mathcal{U}[0,1]$ %// Generate a sample from a uniform distribution.
		\While{True}
		\State Solve $u=P_{i+1}(s-t+P_{i+1}^{-1}(y))$ for $s$
		\If{$s \in [s_i,s_{i+1})$}
		\State \textbf{return} $s$
		\Else
		\State $y = P_{i+1}(s_{i+1} -t + P_{i+1}^{-1}(y))			$
		\State $t=s_{i+1}$
		\State $i = i+1$
		\EndIf
		\EndWhile
		\EndFunction
	\end{algorithmic}
	\label{alg:singlesamplegeneral}
\end{algorithm}



As illustrated in Figure \ref{fig:traverse}, we assume a setting where a particle traverses different materials, each defined by $p_{i}(s)$ and $P_i(s)$. The distance from $\bx$ to the $i-$th interface is given by $s_i$, with $s_0=0$.
For $s\in[0,s_1)$, we write
\begin{equation}
	\begin{cases}
		P(0,\bx,\bOmega)  & = 0,                                                                                 \\
		\frac{d}{ds}P(s,\bx,\bOmega) & = p_{\bx(s)}(P_{\bx(s)}^{-1}P(s,\bx,\bOmega))) \\
		&= p_1(p_1^{-1}(P(s,\bx,\bOmega))) = {1}/{(P_1^{-1})'(P(s,\bx,\bOmega))}.
	\end{cases}
\end{equation}
The last line is equivalent to
\begin{align}
	(P_1^{-1})'(P(s,\bx,\bOmega))\cdot \frac{d}{ds}P(s,\bx,\bOmega) =  \frac{d}{ds} P_1^{-1}(P(s,\bx,\bOmega)).
\end{align}
Thus,
\begin{subequations}
\begin{align}
	P_1^{-1}(P(s,\bx,\bOmega))       & = s+c_1       \\
	\Leftrightarrow P(s,\bx,\bOmega) & = P_1(s+c_1).
\end{align}
\end{subequations}
Since $P(0,\bx,\bOmega) = 0$, we have $c_1=0$, \ie $P(s,\bx,\bOmega) = P_1(s)$ for $s \in [0,s_1)$.
Next, assume a particle to be at the interface between material $i$ and $i+1$, having already traveled a distance $s_{i}$.
The value $P(\bx,\bOmega,s_{i})=y_i$ is known.
For $s \in [s_i, s_{i+1})$ we write
\begin{equation}
	\begin{cases}
		P(s_i,\bx,\bOmega) & = y_{i},                           \\
		\frac{d}{ds}P(s,\bx,\bOmega)  & = {1}/{(P_{i+1}^{-1})'(P(s,\bx,\bOmega))},
	\end{cases}
\end{equation}
or with $\tilde{P}(\tilde{s},\bx,\bOmega)=
P(s_i+\tilde{s},\bx,\bOmega)= P(s,\bx,\bOmega)$
\begin{equation}
	\begin{cases}
		\tilde{P}(0,\bx,\bOmega)          & = y_{i},                                           \\
		\frac{d}{d\tilde{s}}\tilde{P}(\tilde{s},\bx,\bOmega) & = {1}/{(P_{i+1}^{-1})'(\tilde{P}(\tilde{s},\bx,\bOmega))}.
	\end{cases}
\end{equation}
We solve for $\tilde{P}(\tilde{s},\bx,\bOmega)$ to get
\begin{align}
	\tilde{P}(\tilde{s},\bx,\bOmega)= P_{i+1}(\tilde{s}+c_{i+1})
\end{align}
Then, $c_{i+1} = P_{i+1}^{-1}(y_{i})$ and consequently
\begin{subequations}
\begin{align}
	P(s,\bx,\bOmega) & = \tilde{P}(\tilde{s},\bx,\bOmega)                   \\
	& =P_{i+1}(\tilde{s}+P_{i+1}^{-1}(y_{i}))  \\
	& =P_{i+1}(s-s_i+P_{i+1}^{-1}(P(s_i,\bx,\bOmega))),
	\label{eq:sampleiterartively}
\end{align}
\end{subequations}
for $s\in[s_{i},s_{i+1})$.
With $P(s)=P_{i+1}(s-s_i+P_{i+1}^{-1}(P(s_i,\bx,\bOmega)))$ inverse transform sampling is easy: Finding $s^*$ such that $P(s^*,\bx,\bOmega) = u$ can be solved by iteratively evaluating the expression in \eqref{eq:sampleiterartively} and checking whether the argument of $P_{i+1}$ lies within $[s_i, s_{i+1})$. The procedure is written out as pseudocode in Algorithm \ref{alg:singlesamplegeneral}.



\input{Chapters/Chapter4/hettransport}
%\section{The need for experimental evidence}
%we need experiments
%whether correlated / uncorrelated is necessary : i don't know
